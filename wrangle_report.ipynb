{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle & Analyze WeRateDogs Data\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Wrangle Report\n",
    "\n",
    "### Gathering Data\n",
    "\n",
    "Data for the project was gather from various sources:\n",
    "\n",
    "* Enhanced Twitter Archive - This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017. Data is loaded from the csv file into a pandas data frame. Dataframe size is 2356 rows and 16 columns. The tweeter ID column is used as an index.  \n",
    "<br/>\n",
    "* Image Predictions File - This file contains top three predictions of dog breed for each dog image from the WeRateDogs Enhanced Twitter Archive. The table contains the top three predictions, tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images). Data is downloaded programmatically from the URL address into a tsv file. The content of the tsv file is then loaded into the pandas' data frame, with size is 2075 rows and 11 columns. The tweeter ID column is used as an index.  \n",
    "<br/>\n",
    "* Twitter API File - Retweet count and favorite count are two of the notable column omissions of the Twitter data archive. Fortunately, this additional data can be gathered from Twitter's API. Twitter API file contains tweet id, favorite count and retweet count. Data is loaded from the txt file into a pandas data frame. Dataframe size is 2354 rows and 2 columns. The tweeter ID column is used as an index. \n",
    "\n",
    "\n",
    "### Assessing Data\n",
    "\n",
    "After gathering, the data is assessed for tidyness and quality:\n",
    "\n",
    "* Enhanced Twitter Archive - As a first step, a sample of data is assessed visually and a summary of data types and non-null values is displayed. This allows to identify columns with the incorrect data type and/or null values. Then, IDs are checked for duplicates. Next, the number of tweets which are replies and retweets is calculated - these types of tweets will not be used in the analysis part. Source column is checked by displaying all existing values, while the quality of the data in the text column is assessed visually by displaying a sample. Expanded URLs are firstly assessed visually and then checked programmatically for the existence of two or more URLs in one cell. Rating denominator is assessed visually by displaying a sample of data, and then ratings with denominator greater than 10 are printed out for further investigation. Rating numerator is also assessed visually. Based on the visual assessment of rating columns, we check programmatically text column for any float ratings. Name of dog column is assessed visually and then programmatically checked for stop words. Dog category (stage) column is checked for the number of values. As the last step, all tweets are checked for dogs with more than one dog category (stage) assigned.\n",
    "\n",
    "        Quality and tidiness issues detected:\n",
    "            - some of the gathered tweets are replies and should be removed;\n",
    "            - the timestamp has an incorrect datatype - is an object, should be DateTime;\n",
    "            - source is an HTML element - its text should be extracted;\n",
    "            - some rows in the text column begin from 'RT @dog_rates:';\n",
    "            - some rows in the text column have leading and/or trailing whitespace;\n",
    "            - some of the gathered tweets are retweets;\n",
    "            - we have 59 missing expanded urls;\n",
    "            - we have 639 expanded urls which contain more than one url address;\n",
    "            - denominator of some ratings is not 10;\n",
    "            - numerator of some ratings is greater than 10 (does not need to be cleaned);\n",
    "            - float ratings have been incorrectly read from the text of tweet;\n",
    "            - 'None' in the name should be convert to NaN;\n",
    "            - we have stop words in the name column;\n",
    "            - dog 'stage' classification (doggo, floofer, pupper or puppo) should be one column;\n",
    "            - some dogs have more than one category assigned;  \n",
    "\n",
    "<br/>\n",
    "\n",
    "* Image Predictions - As a first step, a sample of data is assessed visually and a summary of data types and non-null values is displayed. This allows to identify columns with the incorrect data type and/or null values. Then, IDs are checked for duplicates. Next, the jpg_url column is checked to confirm if it contains only jpg and png images. As the last step, the 1st prediction is checked to see how many images have been classified as dog images.\n",
    "\n",
    "        Quality and tidiness issues detected:\n",
    "            - the dataset has 2075 entries, while Enhanced Twitter Archive dataset has 2356 entries;\n",
    "            - column names are confusing and do not give much information about the content;\n",
    "            - dog breeds contain underscores, and have different case formatting;\n",
    "            - only 2075 images have been classified as dog images for top prediction;\n",
    "            - dataset should be merged with the Enhanced Twitter Archive dataset;  \n",
    "            \n",
    "<br/>\n",
    "\n",
    "* Twitter API Data - As a first step, a sample of data is assessed visually and a summary of data types and non-null values is displayed. This allows to identify columns with the incorrect data type and/or null values. Then, IDs are checked for duplicates.\n",
    "\n",
    "        Quality and tidiness issues detected:\n",
    "            - twitter archive dataset has 2356 entries, whileEnhanced Twitter Archive dataset has 2354;\n",
    "            - the dataset should be merged with the Enhanced Twitter Archive dataset;\n",
    "\n",
    "### Cleaning Data\n",
    "\n",
    "The quality and tidiness issues identified in the Assessing Data section are cleaned using pandas:\n",
    "\n",
    "* Enhanced Twitter Archive - As a first step, a copy of dataset is created for use throughout the cleaning exercise. As some of the gathered tweets are replies and retweets, we remove them together with other associated columns. Next, we fix the timestamp which has an incorrect data type - is an object - by converting it to DateTime. Data in the source column is cleaned by extracting inner text from the provided HTML element. As some rows have to lead and/or trailing whitespace, we strip whitespace. We have 639 expanded URLs which contain more than one URL address and 59 missing expanded URLs, therefore we build correct links by using the tweet id field. Float ratings, which have been incorrectly read from the text of tweet are gathered again, this time correctly. The denominator of some ratings is not 10, while numerator of some ratings is greater than 10 - the fact that the rating numerators are greater than the denominators does not need to be cleaned, however, we introduce a normalized rating which will be used for plots. As we have stop words and 'None' values in the Name column, we replace stop words with the correct name and replace None with Nans. Dog 'stage' classification (doggo, floofer, pupper or puppo) which was broken into four separate columns, is merged into one column.\n",
    "\n",
    "<br/>\n",
    "\n",
    "* Image Predictions - As a first step, a copy of dataset is created for use throughout the cleaning exercise. As some of the column names are confusing and do not give much information about the content, we rename columns. Then we clean dog breeds - we replace underscores with whitespace and capitalize the first letter to have consistent and clean formatting. Since only 2075 images have been classified as dog images for the top prediction (1st prediction), we use the dog breed predicted in the 2nd or 3rd predictions for the remaining rows.\n",
    "\n",
    "<br/>\n",
    "\n",
    "* Twitter API Data - There is no need to perform cleaning tasks in this data set.\n",
    "\n",
    "As a last step of the cleaning process, we merge all datasets into one and export to csv file.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
